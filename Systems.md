# Systems

A system is anything with multiple parts that depend on each other. In other words, every machine and activity is a system on some level. Systems are the best way to achieve [[Goals]]. Everything is a system, and is also part of a larger system.

## Interesting Systems Properties

- [[Modularity]].
- Responsiveness.
  - Know what the system is doing and make the [[Feedback Loops]] fast.
- [[Decentralized Protocols|Decentralized]].
- Permissionless.

## Changing Systems

To [change a system](https://intenseminimalism.com/2015/a-framework-for-thinking-about-systems-change/) you need vision, skills, [[Incentives|incentives]], resources and an action plan. Changing a [complex system](https://complexityexplained.github.io/) is hard and [even if the intention is good, the result might not](https://fs.blog/2013/10/iatrogenics/).

First, focus on [[Incentives]]. [Don't be angry at the people who are benefiting from a system, or at the system itself](https://news.ycombinator.com/item?id=22043088). Most just end up that way, the same way a river meanders towards the sea, or an electrical current tries to find ground. Modeling human systems is hard because humans also respond to models of their world and then change it. They are [reflexive](https://en.wikipedia.org/wiki/Reflexivity_(social_theory)).

Keep in mind intervening in a system requires some kind of theory, some kind of model where the positive effects will definitely be better than the side effects - and given how little we know and how bad we are at prediction, this will probably be wrong. A great way to start is removing things, kind of like a negative intervention, and so probably good (e.g: you're unlikely to find a medicine as helpful as smoking is harmful, so focus on stopping smoking). Easy to replace systems get replaced by difficult to replace systems. Sometimes is better to have fewer points of small disruptive change, but make a larger one much more meaningful.

[A complex system that works is invariably found to have evolved from a simple system that worked](https://en.wikipedia.org/wiki/John_Gall_(author)#Gall's_law) (more [elementary systems functions](https://en.wikipedia.org/wiki/Systemantics#Elementary_systems_functions)). [Systems want to grow and grow](https://stephango.com/remove), but without pruning, they collapse. A good system is designed to be periodically cleared of cruft. It has a built-in counterbalance.

Complex systems usually have [attractor landscapes](https://ncase.me/attractors/) that can be used to change it. [The world is richer and more complicated than we give it credit for](https://slatestarcodex.com/2017/03/16/book-review-seeing-like-a-state/). To simplify it, we can focus on elements at different scales that have scale decoupling (quantum mechanics is decoupled from philosophy).

Evolution is easier than revolution. A good approach to incrementally change a system (similar to [[Evolution|natural selection]]) is to:

1. Start by identifying the highest-leverage level to optimize at: Ask whether you're optimizing the machine or a cog within it. Complex systems might change in unexpected ways (butterfly effects). Minor differences in starting points make big differences on future states.
2. Begin optimizing the system by following the [Theory of Constraints](https://en.wikipedia.org/wiki/Theory_of_constraints): At any time, just one of a system's inputs is constraining its other inputs from achieving a greater total output. Make incremental changes. [Understand how the inputs affect the outputs of the system](https://faintsignal.org/pressure-to-meet-a-target-value-changes-the-system-or-the-data/#fn2). Alter the incentive landscape. [If you can make your system less miserable, make your system less miserable!](https://astralcodexten.substack.com/p/book-review-the-cult-of-smart)
3. Re-examine the system from the ground up. Get data. Take nothing but the proven, underlying principles as given. Work up from there to create something better.

These are places within a complex system (a corporation, an economy, a living body, a city, an ecosystem) where a small shift in one thing can produce big changes in everything. These are [the places to intervene in a system](https://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/) (in increasing order of effectiveness):

1. Constants, parameters, numbers (such as subsidies, taxes, standards).
2. The sizes of buffers and other stabilizing stocks, relative to their flows.
3. The structure of material stocks and flows (such as transport networks, population age structures).
4. The lengths of delays, relative to the rate of system change.
5. The strength of negative feedback loops, relative to the impacts they are trying to correct against.
6. The gain around driving positive feedback loops.
7. The structure of information flows (who does and does not have access to information).
8. The rules of the system (such as incentives, punishments, constraints).
9. The power to add, change, evolve, or self-organize system structure.
10. The goals of the system.
11. The mindset or paradigm out of which the system — its goals, structure, rules, delays, parameters — arises.
12. The power to transcend paradigms.

**Don't aim for an ideal system. Build a set of [[processes]] and protocols that evolve to fit the environment over time.** [Complex systems fail](https://how.complexsystems.fail/). Don't move the ball, move the hill and fight loops with loops. [High entropy systems are easier to maintain and require less energy to keep things as they are](https://slatestarcodex.com/2016/07/25/how-the-west-was-won/).

[The purpose of a system is what it does](https://en.m.wikipedia.org/wiki/The_purpose_of_a_system_is_what_it_does). However, [this view can be misleading](https://www.astralcodexten.com/p/come-on-obviously-the-purpose-of). A system's purpose is often its intended goal, even if it fails or has unintended consequences (e.g., a hospital intends to cure all patients, even if it doesn't succeed; bus emissions are side effects, not the purpose). Attributing failure or negative side effects as the *intended* purpose often ignores complexity, conflicting goals, or simple failure. A [common interpretation](https://www.astralcodexten.com/p/highlights-from-the-comments-on-posiwid) is that if a system consistently fails its stated purpose but isn't changed, its *actual* (perhaps hidden) purpose might be succeeding. Understanding the *intended* purpose, even if the system fails, is often useful for predicting its behavior (e.g., predicting an intelligence agency's actions based on its goal to prevent attacks, even if it fails). The phrase can obscure the useful distinction between a primary goal and unavoidable (or accepted) side effects.

If everyone agrees the current system doesn't work well, who perpetuates it? Some [systems with systemic/incentives failures are broken in multiple places so that no one actor can make them better](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/), even though, in principle, some [magically coordinated action could move to a new stable state](https://equilibriabook.com/molochs-toolbox/).

A system needs competition and [slack](https://slatestarcodex.com/2020/05/12/studies-on-slack/) (the absence of binding constraints on behavior). By having some margin for error, the system is allowed to pursue opportunities and explore approaches that improve it. It needs chaos too. Evolution requires two things; variation (chaos) and selection (loops).

Interaction between system actors causes **externalities**: the consequences of their actions on _other actors or processes_. This is important because, intuitively, humans are self-centered, and it's easy to not notice the effects your actions have on others. And it almost never feels as _visceral_ as the costs and benefits to yourself. The canonical examples are [[coordination]] problems, like climate change. Taking a plane flight has strong benefits to me, but costs everyone on Earth a little bit, a negative externality. And a lot of the problems in the world today boil down to coordination problems where our actions have negative externalities.

Most large social systems are pursuing objectives other than the ones they proclaim, and the ones they pursue are wrong. E.g: [The educational system is not dedicated to produce learning by students, but teaching by teachers—and teaching is a major obstruction to learning.](https://thesystemsthinker.com/a-lifetime-of-systems-thinking/)

[Social systems belong to the class called multi-loop nonlinear feedback systems](https://ocw.mit.edu/courses/15-988-system-dynamics-self-study-fall-1998-spring-1999/65cdf0faf132dec7ec75e91f9651b31f_behavior.pdf). In the long history of evolution it has not been necessary until very recent historical times for people to understand complex feedback systems. Evolutionary processes have not given us the mental ability to interpret properly the dynamic behavior of those complex systems in which we are now embedded.

There are tools that [help understand how complex systems behave](https://youtu.be/PCwtsK_FhUw).

1. [[Feedback Loops]]. Systems are made of loops where things feed back into themselves. Can be reinforcing loops (compound interest) or balancing loops (thermostat).
2. Attractors. States a system naturally "wants" to settle into. Systems can have "bad" attractors.
3. [[Emergence]]. Complex behavior arises from simple rules followed by individuals, not from a central leader. You cannot understand the system just by looking at one piece in isolation.

A [[Mental Models|mental model]] of a system is the reduction of how it works. The model cuts through the noise to highlight the system's core components and how they work together.

Remember, sometimes not doing something is better than doing it ([Primum non nocere](https://en.wikipedia.org/wiki/Primum_non_nocere)). E.g: controlling small fires instead of letting them burn the top layer of the forest. Spending 1 week repairing trains because there was an accident makes people use the car more, turning into more deaths than leaving the train rails as they were.

> _[Almost no one is evil](https://thecompendium.cards/c/everything/sort/stars/assume-theres-no-villain); almost everything is broken._

## Inadequate Equilibria

[An Inadequate Equilibrium is a situation in which a community, an institution, or society at large is in a bad _Nash Equilibrium_](https://equilibriabook.com/molochs-toolbox/). The group as a whole has some sub-optimal set of norms and it would be better off with a different set of norms, but there's no individual actor who has both the power and the incentive to change the norms for the group. So the bad equilibrium persists. These concepts can be sorted in 3 categories:

1. Cases where the decision lies in the hands of people who would gain little personally, or lose out personally, if they did what was necessary to help someone else.
2. Cases where decision-makers can't reliably learn the information they need to make decisions, even though someone else has that information.
3. Systems that are broken in multiple places so that no one actor can make them better, even though, in principle, some magically coordinated action could move to a new stable state. One systemic problem can often be overcome by one altruist in the right place. Two systemic problems are another matter entirely.

### Examples

- Making the switch from not relying on prediction markets to relying on prediction markets is fraught, because it might embarrass the leadership of existing institutions by revealing that their professed estimates are not very credible.
- There are several newly designed voting methods which are likely to be improvements over the current system, but most have seen limited, if any, uptake.
  - It's difficult to change political systems from the outside.
  - Within a two-party system, both benefit from first past the post voting, as they know they have a ~50% chance of winning each election, so there is no incentive for them to change from within.
  - Proponents of voting reform have not yet been able to coordinate on which method they recommend.
- Ongoing over-fishing of ocean fish. Each individual fishery (and, at a higher level, each country) would prefer a world where everyone fishes a sustainable amount, rather than over-fishing and crashing the fish populations that they all rely upon, but without a centralized enforcement mechanism, they have no way of ensuring that the other fisheries (or countries) go along with them in cutting back on fishing, so unilaterally doing so would simply make them get out-competed by others.
- Countries building up their militaries. Most of the use of sizable militaries is fighting against other militaries (and as a deterrent against such), so they are overall a negative-sum game. If countries all agreed to cut back their militaries, they would (for the most part) all benefit, but due to the competitive nature, there is a strong incentive to not cut back.
- Using companies producing widgets as an example, each company might wish to fairly pay their workers, maintain a safe work environment, and not pollute the environment. However, other companies can gain an edge by sacrificing things in favor of producing more widgets (e.g. hiring more workers at cheaper wages). Thus, the principled company must make similar changes, or get out-competed. This can continue until the companies have all sacrificed everything they can in favor of more [[productivity]], even if all of them would have preferred to peacefully coexist with comfortable work conditions.
- Doctors being overly cautious in treatment. The [[Incentives]] punish positive mistakes much more heavily than negative ones. In this case, any deviation from what is considered to be the "proper" way of dealing with a case subjects the doctor to risk of being sued for malpractice in a way that sticking to the "proper" method does not, even if the deviation would have been a net positive in expectation for the patient.
